{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c52c8c2a-9b6e-4c88-9ec9-95761d421bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric import seed_everything\n",
    "from imports.TrainModel import TrainModel\n",
    "seed_everything(0)\n",
    "\n",
    "from models.gcn import ya_1\n",
    "model = ya_1().to('cuda')\n",
    "model\n",
    "\n",
    "#!rm ../data/contact_extraction2/{train,test,val}/processed/*\n",
    "\n",
    "tm = TrainModel('/home/hussain/tactile/data/contact_extraction2/', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2de8acc-4b60-48ff-81d5-85ae07c965b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                               | 0/407 [00:00<?, ?batch/s]/home/hussain/tactile/src/imports/TrainModel.py:53: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n",
      "/home/hussain/tactile/src/models/gcn.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data.x = F.sigmoid(self.conv1(torch.tensor(data.x.view(-1,1), dtype=torch.float32), data.edge_index) )#, data.edge_attr ))\n",
      "/home/hussain/miniconda3/envs/tactile38/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/hussain/miniconda3/envs/tactile38/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 0:  17%|▎ | 69/407 [00:17<01:28,  3.84batch/s, train_loss=0.00344]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tactile/src/imports/TrainModel.py:59\u001b[0m, in \u001b[0;36mTrainModel.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m end_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data)\n\u001b[1;32m     58\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(end_point[\u001b[38;5;241m0\u001b[39m], data\u001b[38;5;241m.\u001b[39my[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile38/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "420ac226-7ea5-4b38-8221-815151d4183c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/hussain/tactile/data/contact_extraction2')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = '/home/hussain/tactile/data/contact_extraction2/'\n",
    "Path(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baa75cdc-3069-4bc3-a4d9-3080563f9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "train_data_aug = T.Compose([T.Cartesian(cat=False), T.RandomFlip(axis=0, p=0.3), T.RandomScale([0.95,0.999]) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82af75b9-e06c-4753-94b2-dda1f88b5a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ya_1(\n",
       "  (conv1): GCNConv(1, 64)\n",
       "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): GCNConv(128, 512)\n",
       "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): GCNConv(512, 256)\n",
       "  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): GCNConv(256, 64)\n",
       "  (bn5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv6): GCNConv(64, 32)\n",
       "  (bn6): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv7): GCNConv(32, 16)\n",
       "  (bn7): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv8): GCNConv(16, 2)\n",
       "  (bn8): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.gcn import ya_1\n",
    "model = ya_1().to('cuda')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb522a8-117e-4542-ac79-fb3c7e6cec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[10197, 16], edge_index=[2, 326304], y=[6], pos=[10197, 3], edge_attr=[326304, 3], batch=[10197], ptr=[4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "990e4f9b-7caf-4b06-aed6-bc1e61d6a0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a631f49d-7e82-4a21-ae7e-32694ec849bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[10197, 1], edge_index=[2, 326304], y=[6], pos=[10197, 3], edge_attr=[326304, 3], batch=[10197], ptr=[4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(tdloader)).to('cuda')\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa18b126-0fbf-419e-a533-11bed5f9332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hussain/tactile/src/models/gcn.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data.x = F.sigmoid(self.conv1(torch.tensor(data.x.view(-1,1), dtype=torch.float32), data.edge_index) )#, data.edge_attr ))\n",
      "/home/hussain/miniconda3/envs/tactile38/lib/python3.8/site-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0682,  0.2231],\n",
       "        [-0.1260,  0.5034],\n",
       "        [ 0.1214,  0.2601]], device='cuda:0', grad_fn=<EluBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_data_aug(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e0bb6d-3256-4f46-aa19-9c3a7cc37873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954004a-f93c-4a4c-85ad-2bf2541860c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.x.view(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c085c-ce5f-4104-8cff-1956890a5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eded9c55-dd3b-434f-911c-29a7418a5966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.4907e-02, -8.5496e-18], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557b9ba-c92c-492d-936c-b90487f30597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SplineConv, voxel_grid, max_pool, max_pool_x\n",
    "\n",
    "sconv = SplineConv(1, 64, dim=3, kernel_size=5)\n",
    "sconv(sample.x.view(-1, 1), sample.edge_index, sample.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44ef0c-ee4c-4622-9766-5ea986511115",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('/home/hussain/data/event_based_sign_lang/test/processed/a/a_0000.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bd020-9a9c-496c-a214-1bdce455b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50e46ef8-3986-4599-8535-03024507a621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[4, 1], edge_index=[2, 3], pos=[4, 2], edge_attr=[3, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "import numpy as np\n",
    "\n",
    "pos = np.array(pos).astype(np.float32)\n",
    "x = np.array([1, 1, 1, 1]).reshape(-1, 1).astype(np.float32)\n",
    "edge_index = np.array([[0, 1], [1, 2], [2, 3]]).T\n",
    "\n",
    "pos = torch.tensor(pos)\n",
    "x = torch.tensor(x)\n",
    "edge_index = torch.tensor(edge_index)\n",
    "data = pyg.data.Data(x, edge_index, pos = pos)\n",
    "pseudo = pyg.transforms.Cartesian()(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4259912-0d3a-47ce-9d2d-045c6e170216",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0328284-56c2-482e-81f1-5da78ce71700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9291],\n",
       "        [1.1216],\n",
       "        [0.9291],\n",
       "        [0.9291]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "gc = GCNConv(1, 1)\n",
    "gc(data.x, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e7d35dc2-584c-430f-85fa-6b0cc5b59188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.9291]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in gc.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "870e7e65-78a8-4611-9767-7269f95ecaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3677ec9afca641dc952b168cc4765479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f809515361024ea29ea3d3faeb11d233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cfe9cfafae444a99d7e4fbaca8a1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0e9c38dcd947cf9fae53080ee06204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ITERATION - loss: 0.00:   0%|                  | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "findCaller() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mrun(train_loader, max_epochs\u001b[38;5;241m=\u001b[39mepochs)\n\u001b[1;32m    102\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 105\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(train_batch_size, val_batch_size, epochs, lr, momentum, log_interval)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@trainer\u001b[39m\u001b[38;5;241m.\u001b[39mon(Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED \u001b[38;5;241m|\u001b[39m Events\u001b[38;5;241m.\u001b[39mCOMPLETED)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_time\u001b[39m(engine):\n\u001b[1;32m     99\u001b[0m     tqdm\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mlast_event_name\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[trainer\u001b[38;5;241m.\u001b[39mlast_event_name\u001b[38;5;241m.\u001b[39mname]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 101\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile38/lib/python3.8/site-packages/ignite/engine/engine.py:874\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length, seed)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;66;03m# Reset generator if previously used\u001b[39;00m\n\u001b[1;32m    873\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEngine run starting with max_epochs=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    876\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    877\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run resuming from iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    878\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m until \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mmax_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile38/lib/python3.8/logging/__init__.py:1446\u001b[0m, in \u001b[0;36mLogger.info\u001b[0;34m(self, msg, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;124;03mLog 'msg % args' with severity 'INFO'.\u001b[39;00m\n\u001b[1;32m   1439\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;124;03mlogger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\u001b[39;00m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misEnabledFor(INFO):\n\u001b[0;32m-> 1446\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINFO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile38/lib/python3.8/logging/__init__.py:1577\u001b[0m, in \u001b[0;36mLogger._log\u001b[0;34m(self, level, msg, args, exc_info, extra, stack_info, stacklevel)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _srcfile:\n\u001b[1;32m   1573\u001b[0m     \u001b[38;5;66;03m#IronPython doesn't track Python frames, so findCaller raises an\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m     \u001b[38;5;66;03m#exception on some versions of IronPython. We trap it here so that\u001b[39;00m\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m#IronPython can use logging.\u001b[39;00m\n\u001b[1;32m   1576\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1577\u001b[0m         fn, lno, func, sinfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindCaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstack_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1578\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m: \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m   1579\u001b[0m         fn, lno, func \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(unknown file)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(unknown function)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: findCaller() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ignite.engine import create_supervised_evaluator, create_supervised_trainer, Events\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.utils import setup_logger\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "def get_data_loaders(train_batch_size, val_batch_size):\n",
    "    data_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        MNIST(download=True, root=\".\", transform=data_transform, train=True), batch_size=train_batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        MNIST(download=False, root=\".\", transform=data_transform, train=False), batch_size=val_batch_size, shuffle=False\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def run(train_batch_size, val_batch_size, epochs, lr, momentum, log_interval):\n",
    "    train_loader, val_loader = get_data_loaders(train_batch_size, val_batch_size)\n",
    "    model = Net()\n",
    "    device = \"cpu\"\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "\n",
    "    model.to(device)  # Move model before creating optimizer\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    criterion = nn.NLLLoss()\n",
    "    trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "    trainer.logger = setup_logger(\"trainer\")\n",
    "\n",
    "    val_metrics = {\"accuracy\": Accuracy(), \"nll\": Loss(criterion)}\n",
    "    evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)\n",
    "    evaluator.logger = setup_logger(\"evaluator\")\n",
    "\n",
    "    pbar = tqdm(initial=0, leave=False, total=len(train_loader), desc=f\"ITERATION - loss: {0:.2f}\")\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
    "    def log_training_loss(engine):\n",
    "        pbar.desc = f\"ITERATION - loss: {engine.state.output:.2f}\"\n",
    "        pbar.update(log_interval)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(engine):\n",
    "        pbar.refresh()\n",
    "        evaluator.run(train_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_accuracy = metrics[\"accuracy\"]\n",
    "        avg_nll = metrics[\"nll\"]\n",
    "        tqdm.write(\n",
    "            f\"Training Results - Epoch: {engine.state.epoch} Avg accuracy: {avg_accuracy:.2f} Avg loss: {avg_nll:.2f}\"\n",
    "        )\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(engine):\n",
    "        evaluator.run(val_loader)\n",
    "        metrics = evaluator.state.metrics\n",
    "        avg_accuracy = metrics[\"accuracy\"]\n",
    "        avg_nll = metrics[\"nll\"]\n",
    "        tqdm.write(\n",
    "            f\"Validation Results - Epoch: {engine.state.epoch} Avg accuracy: {avg_accuracy:.2f} Avg loss: {avg_nll:.2f}\"\n",
    "        )\n",
    "\n",
    "        pbar.n = pbar.last_print_n = 0\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED | Events.COMPLETED)\n",
    "    def log_time(engine):\n",
    "        tqdm.write(f\"{trainer.last_event_name.name} took { trainer.state.times[trainer.last_event_name.name]} seconds\")\n",
    "\n",
    "    trainer.run(train_loader, max_epochs=epochs)\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "run(train_batch_size=10, val_batch_size=10, epochs=20, lr=0.01, momentum=0.5, log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5f1cbac5-11a9-48e7-8116-bcc4cfe9317f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-ignite\n",
      "  Downloading pytorch_ignite-0.4.10-py3-none-any.whl (264 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /home/hussain/miniconda3/envs/tactile38/lib/python3.8/site-packages (from pytorch-ignite) (1.12.1)\n",
      "Requirement already satisfied: packaging in /home/hussain/miniconda3/envs/tactile38/lib/python3.8/site-packages (from pytorch-ignite) (21.3)\n",
      "Requirement already satisfied: typing_extensions in /home/hussain/miniconda3/envs/tactile38/lib/python3.8/site-packages (from torch<2,>=1.3->pytorch-ignite) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/hussain/miniconda3/envs/tactile38/lib/python3.8/site-packages (from packaging->pytorch-ignite) (3.0.9)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.4.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-ignite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159a3a7-cd3c-4962-a518-327e7002cbbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
