{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ab0f100-7295-42f2-9fe1-503f03f759ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader\n",
    "from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "loader = ChickenpoxDatasetLoader()\n",
    "\n",
    "dataset = loader.get_dataset()\n",
    "\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c4cec8-6e73-4c31-aea4-357274dc6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.recurrent = DCRNN(node_features, 32, 1)\n",
    "        self.linear = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h = self.recurrent(x, edge_index, edge_weight)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d66f0b-260a-439f-8d63-7742fd25250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e54e9cb7af447cbae80d1ba34a9410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "model = RecurrentGCN(node_features = 4).cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(200)):\n",
    "    cost = 0\n",
    "    for time, snapshot in enumerate(train_dataset):\n",
    "        snapshot.cuda()\n",
    "        y_hat = model(snapshot.x.cuda(), snapshot.edge_index.cuda(), snapshot.edge_attr.cuda())\n",
    "        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
    "    cost = cost / (time+1)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "072465da-629a-4385-bf50-7577f2907165",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMovingMNIST\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MovingMNIST\n\u001b[1;32m      4\u001b[0m train_set \u001b[38;5;241m=\u001b[39m MovingMNIST(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../data/mnist\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m      7\u001b[0m                  dataset\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[1;32m      8\u001b[0m                  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      9\u001b[0m                  shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'legacy'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from legacy.MovingMNIST import MovingMNIST\n",
    "\n",
    "train_set = MovingMNIST(root='../../data/mnist', train=True, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=100,\n",
    "                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba39c6f9-b5e8-49f5-a674-3d90a890926e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "136f9425-aaeb-4d7c-998e-20e7af7773bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MovingMNIST\n",
       "    Number of datapoints: 9000\n",
       "    Train/test: train\n",
       "    Root Location: ../../data/mnist\n",
       "    Transforms (if any): None\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb3abd3a-fea3-4349-bc09-14f483822417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f706e241cd0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.io import *\n",
    "\n",
    "imshow(train_set[0][0][9].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "406cd5b0-68cb-4312-9f6f-1de7395c3883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hussain/me/projects/tactile/src/legacy/.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 197\u001b[0m\n\u001b[1;32m    194\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 197\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[37], line 193\u001b[0m, in \u001b[0;36mConvLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 193\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[37], line 134\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m layer:\n\u001b[0;32m--> 134\u001b[0m         B, S, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    135\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mS, C, H, W)\n\u001b[1;32m    136\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer)(x)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "root_dir = os.path.join(os.getcwd(), '.')\n",
    "print(root_dir)\n",
    "\n",
    "class Config:\n",
    "    gpus = [0, ]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if torch.cuda.is_available():\n",
    "        num_workers = 8 * len(gpus)\n",
    "        train_batch_size = 32\n",
    "        valid_batch_size = 2 * train_batch_size\n",
    "        test_batch_size = 2 * train_batch_size\n",
    "    else:\n",
    "        num_workers = 0\n",
    "        train_batch_size = 2\n",
    "        valid_batch_size = 2 * train_batch_size\n",
    "        test_batch_size = 2 * train_batch_size\n",
    "    data_file = 'datas/train-images-idx3-ubyte.gz'\n",
    "\n",
    "    num_frames_input = 10\n",
    "    num_frames_output = 10\n",
    "    image_size = (28, 28)\n",
    "    input_size = (64, 64)\n",
    "    step_length = 0.1\n",
    "    num_objects = [3]\n",
    "    display = 10\n",
    "    draw = 10\n",
    "    train_dataset = (0, 10000)\n",
    "    valid_dataset = (10000, 12000)\n",
    "    test_dataset = (12000, 15000)\n",
    "    epochs = 100\n",
    "\n",
    "    # (type, activation, in_ch, out_ch, kernel_size, padding, stride)\n",
    "    encoder = [('conv', 'leaky', 1, 32, 3, 1, 2),\n",
    "             ('convlstm', '', 32, 32, 3, 1, 1),\n",
    "             ('conv', 'leaky', 32, 64, 3, 1, 2),\n",
    "             ('convlstm', '', 64, 64, 3, 1, 1),\n",
    "             ('conv', 'leaky', 64, 128, 3, 1, 2),\n",
    "             ('convlstm', '', 128, 128, 3, 1, 1)]\n",
    "    decoder = [('deconv', 'leaky', 128, 64, 4, 1, 2),\n",
    "               ('convlstm', '', 128, 64, 3, 1, 1),\n",
    "               ('deconv', 'leaky', 64, 32, 4, 1, 2),\n",
    "               ('convlstm', '', 64, 32, 3, 1, 1),\n",
    "               ('deconv', 'leaky', 32, 32, 4, 1, 2),\n",
    "               ('convlstm', '', 33, 32, 3, 1, 1),\n",
    "               ('conv', 'sigmoid', 32, 1, 1, 0, 1)]\n",
    "\n",
    "    data_dir = os.path.join(root_dir, 'data')\n",
    "    output_dir = os.path.join(root_dir, 'output')\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model_dir = os.path.join(output_dir, 'model')\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    log_dir = os.path.join(output_dir, 'log')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    cache_dir = os.path.join(output_dir, 'cache')\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "class ConvLSTMBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_features, kernel_size=3, padding=1, stride=1):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.conv = self._make_layer(in_channels+num_features, num_features*4,\n",
    "                                       kernel_size, padding, stride)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, kernel_size, padding, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,\n",
    "                      kernel_size=kernel_size, padding=padding, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        :param inputs: (B, S, C, H, W)\n",
    "        :param hidden_state: (hx: (B, S, C, H, W), cx: (B, S, C, H, W))\n",
    "        :return:\n",
    "        '''\n",
    "        outputs = []\n",
    "        B, S, C, H, W = inputs.shape\n",
    "        hx = torch.zeros(B, self.num_features, H, W).to(inputs.device)\n",
    "        cx = torch.zeros(B, self.num_features, H, W).to(inputs.device)\n",
    "        for t in range(S):\n",
    "            combined = torch.cat([inputs[:, t], # (B, C, H, W)\n",
    "                                  hx], dim=1)\n",
    "            gates = self.conv(combined)\n",
    "            ingate, forgetgate, cellgate, outgate = torch.split(gates, self.num_features, dim=1)\n",
    "            ingate = torch.sigmoid(ingate)\n",
    "            forgetgate = torch.sigmoid(forgetgate)\n",
    "            outgate = torch.sigmoid(outgate)\n",
    "\n",
    "            cy = (forgetgate * cx) + (ingate * cellgate)\n",
    "            hy = outgate * torch.tanh(cy)\n",
    "            outputs.append(hy)\n",
    "            hx = hy\n",
    "            cx = cy\n",
    "\n",
    "        return torch.stack(outputs).permute(1, 0, 2, 3, 4).contiguous() # (S, B, C, H, W) -> (B, S, C, H, W)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for idx, params in enumerate(config.encoder):\n",
    "            setattr(self, params[0]+'_'+str(idx), self._make_layer(*params))\n",
    "            self.layers.append(params[0]+'_'+str(idx))\n",
    "\n",
    "    def _make_layer(self, type, activation, in_ch, out_ch, kernel_size, padding, stride):\n",
    "        layers = []\n",
    "        if type == 'conv':\n",
    "            layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=padding, stride=stride, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "            if activation == 'leaky': layers.append(nn.LeakyReLU(inplace=True))\n",
    "            elif activation == 'relu': layers.append(nn.ReLU(inplace=True))\n",
    "        elif type == 'convlstm':\n",
    "            layers.append(ConvLSTMBlock(in_ch, out_ch, kernel_size=kernel_size, padding=padding, stride=stride))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (B, S, C, H, W)\n",
    "        :return:\n",
    "        '''\n",
    "        outputs = [x]\n",
    "        for layer in self.layers:\n",
    "            if 'conv_' in layer:\n",
    "                B, S, C, H, W = x.shape\n",
    "                x = x.view(B*S, C, H, W)\n",
    "            x = getattr(self, layer)(x)\n",
    "            if 'conv_' in layer: x = x.view(B, S, x.shape[1], x.shape[2], x.shape[3])\n",
    "            if 'convlstm' in layer: outputs.append(x)\n",
    "        return outputs\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = []\n",
    "        for idx, params in enumerate(config.decoder):\n",
    "            setattr(self, params[0]+'_'+str(idx), self._make_layer(*params))\n",
    "            self.layers.append(params[0]+'_'+str(idx))\n",
    "\n",
    "    def _make_layer(self, type, activation, in_ch, out_ch, kernel_size, padding, stride):\n",
    "        layers = []\n",
    "        if type == 'conv':\n",
    "            layers.append(nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size, padding=padding, stride=stride, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "            if activation == 'leaky': layers.append(nn.LeakyReLU(inplace=True))\n",
    "            elif activation == 'relu': layers.append(nn.ReLU(inplace=True))\n",
    "            elif activation == 'sigmoid': layers.append(nn.Sigmoid())\n",
    "        elif type == 'convlstm':\n",
    "            layers.append(ConvLSTMBlock(in_ch, out_ch, kernel_size=kernel_size, padding=padding, stride=stride))\n",
    "        elif type == 'deconv':\n",
    "            layers.append(nn.ConvTranspose2d(in_ch, out_ch, kernel_size=kernel_size, padding=padding, stride=stride, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_ch))\n",
    "            if activation == 'leaky': layers.append(nn.LeakyReLU(inplace=True))\n",
    "            elif activation == 'relu': layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, encoder_outputs):\n",
    "        '''\n",
    "        :param x: (B, S, C, H, W)\n",
    "        :return:\n",
    "        '''\n",
    "        idx = len(encoder_outputs)-1\n",
    "        for layer in self.layers:\n",
    "            if 'conv_' in layer or 'deconv_' in layer:\n",
    "                x = encoder_outputs[idx]\n",
    "                B, S, C, H, W = x.shape\n",
    "                x = x.view(B*S, C, H, W)\n",
    "                x = getattr(self, layer)(x)\n",
    "                x = x.view(B, S, x.shape[1], x.shape[2], x.shape[3])\n",
    "            elif 'convlstm' in layer:\n",
    "                idx -= 1\n",
    "                x = torch.cat([encoder_outputs[idx], x], dim=2)\n",
    "                x = getattr(self, layer)(x)\n",
    "                encoder_outputs[idx] = x\n",
    "        return x\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "model(train_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bbdb29b-5f3c-4f80-bc36-69e515a1874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
    "        \"\"\"\n",
    "        Initialize ConvLSTM cell.\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim: int\n",
    "            Number of channels of input tensor.\n",
    "        hidden_dim: int\n",
    "            Number of channels of hidden state.\n",
    "        kernel_size: (int, int)\n",
    "            Size of the convolutional kernel.\n",
    "        bias: bool\n",
    "            Whether or not to add the bias.\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "\n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "\n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, image_size):\n",
    "        height, width = image_size\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
    "\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        num_layers: Number of LSTM layers stacked on each other\n",
    "        batch_first: Whether or not dimension 0 is the batch or not\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "        Note: Will do same padding.\n",
    "    Input:\n",
    "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
    "    Output:\n",
    "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    Example:\n",
    "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
    "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
    "        >> _, last_states = convlstm(x)\n",
    "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
    "                 batch_first=False, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "\n",
    "        self._check_kernel_size_consistency(kernel_size)\n",
    "\n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
    "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
    "            raise ValueError('Inconsistent list length.')\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "\n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                          hidden_dim=self.hidden_dim[i],\n",
    "                                          kernel_size=self.kernel_size[i],\n",
    "                                          bias=self.bias))\n",
    "\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_tensor: todo\n",
    "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
    "        hidden_state: todo\n",
    "            None. todo implement stateful\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        if not self.batch_first:\n",
    "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
    "\n",
    "        b, _, _, h, w = input_tensor.size()\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            # Since the init is done in forward. Can send image size here\n",
    "            hidden_state = self._init_hidden(batch_size=b,\n",
    "                                             image_size=(h, w))\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "\n",
    "        seq_len = input_tensor.size(1)\n",
    "        cur_layer_input = input_tensor\n",
    "\n",
    "        for layer_idx in range(self.num_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        return layer_output_list, last_state_list\n",
    "\n",
    "    def _init_hidden(self, batch_size, image_size):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
    "        return init_states\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_size):\n",
    "        if not (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "36f0e3e9-bd17-4a1b-912b-6534387562ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[37], line 193\u001b[0m, in \u001b[0;36mConvLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 193\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[37], line 134\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m layer:\n\u001b[0;32m--> 134\u001b[0m         B, S, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    135\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mS, C, H, W)\n\u001b[1;32m    136\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer)(x)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(model(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7434c555-58af-4e13-b923-387d1425d2e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mConvLSTMBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[40], line 87\u001b[0m, in \u001b[0;36mConvLSTMBlock.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m:param inputs: (B, S, C, H, W)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m:param hidden_state: (hx: (B, S, C, H, W), cx: (B, S, C, H, W))\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     86\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 87\u001b[0m B, S, C, H, W \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     88\u001b[0m hx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features, H, W)\u001b[38;5;241m.\u001b[39mto(inputs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     89\u001b[0m cx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features, H, W)\u001b[38;5;241m.\u001b[39mto(inputs\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "ConvLSTMBlock(1, 4)(train_set[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d88f1e0-0115-43f0-b1d6-00eaa825d3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvLSTM(\n",
       "  (encoder): Encoder(\n",
       "    (conv_0): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (convlstm_1): Sequential(\n",
       "      (0): ConvLSTMBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (convlstm_3): Sequential(\n",
       "      (0): ConvLSTMBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_4): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (convlstm_5): Sequential(\n",
       "      (0): ConvLSTMBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (deconv_0): Sequential(\n",
       "      (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (convlstm_1): Sequential(\n",
       "      (0): ConvLSTMBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (deconv_2): Sequential(\n",
       "      (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (convlstm_3): Sequential(\n",
       "      (0): ConvLSTMBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (deconv_4): Sequential(\n",
       "      (0): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "    )\n",
       "    (convlstm_5): Sequential(\n",
       "      (0): ConvLSTMBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(65, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_6): Sequential(\n",
       "      (0): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "34b1c1fd0608eac91ff8ee3b378baa82baa76b865f0fb1b05cc06f1667b6717e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
