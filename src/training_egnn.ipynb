{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9544966e-4466-4d33-ad7d-bb7b469f2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class E_GCL(nn.Module):\n",
    "    \"\"\"\n",
    "    E(n) Equivariant Convolutional Layer\n",
    "    re\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_nf, \n",
    "        output_nf, \n",
    "        hidden_nf, \n",
    "        edges_in_d=0, \n",
    "        act_fn=nn.SiLU(), \n",
    "        residual=True, \n",
    "        attention=False, \n",
    "        normalize=False, \n",
    "        coords_agg='mean', \n",
    "        tanh=False\n",
    "    ):\n",
    "        super(E_GCL, self).__init__()\n",
    "        input_edge = input_nf * 2\n",
    "        self.residual = residual\n",
    "        self.attention = attention\n",
    "        self.normalize = normalize\n",
    "        self.coords_agg = coords_agg\n",
    "        self.tanh = tanh\n",
    "        self.epsilon = 1e-8\n",
    "        edge_coords_nf = 1\n",
    "\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(input_edge + edge_coords_nf + edges_in_d, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, 2*hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(2*hidden_nf, hidden_nf)\n",
    "        )\n",
    "\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_nf + input_nf, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, 2*hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(2*hidden_nf, output_nf)\n",
    "        )\n",
    "\n",
    "        layer = nn.Linear(hidden_nf, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        coord_mlp = []\n",
    "        coord_mlp.append(nn.Linear(hidden_nf, hidden_nf))\n",
    "        coord_mlp.append(act_fn)\n",
    "        coord_mlp.append(layer)\n",
    "        if self.tanh:\n",
    "            coord_mlp.append(nn.Tanh())\n",
    "        self.coord_mlp = nn.Sequential(*coord_mlp)\n",
    "\n",
    "        if self.attention:\n",
    "            self.att_mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_nf, 1),\n",
    "                nn.Sigmoid())\n",
    "\n",
    "    def edge_model(self, source, target, radial, edge_attr):\n",
    "        if edge_attr is None:  # Unused.\n",
    "            out = torch.cat([source, target, radial], dim=1)\n",
    "        else:\n",
    "            out = torch.cat([source, target, radial, edge_attr], dim=1)\n",
    "        out = self.edge_mlp(out)\n",
    "        if self.attention:\n",
    "            att_val = self.att_mlp(out)\n",
    "            out = out * att_val\n",
    "        return out\n",
    "\n",
    "    def node_model(self, x, edge_index, edge_attr, node_attr):\n",
    "        row, col = edge_index\n",
    "        agg = unsorted_segment_sum(edge_attr, row, num_segments=x.size(0))\n",
    "        if node_attr is not None:\n",
    "            agg = torch.cat([x, agg, node_attr], dim=1)\n",
    "        else:\n",
    "            agg = torch.cat([x, agg], dim=1)\n",
    "        out = self.node_mlp(agg)\n",
    "        if self.residual:\n",
    "            out = x + out\n",
    "        return out, agg\n",
    "\n",
    "    def coord_model(self, coord, edge_index, coord_diff, edge_feat):\n",
    "        row, col = edge_index\n",
    "        trans = coord_diff * self.coord_mlp(edge_feat)\n",
    "        if self.coords_agg == 'sum':\n",
    "            agg = unsorted_segment_sum(trans, row, num_segments=coord.size(0))\n",
    "        elif self.coords_agg == 'mean':\n",
    "            agg = unsorted_segment_mean(trans, row, num_segments=coord.size(0))\n",
    "        else:\n",
    "            raise Exception('Wrong coords_agg parameter' % self.coords_agg)\n",
    "        coord = coord + agg\n",
    "        return coord\n",
    "\n",
    "    def coord2radial(self, edge_index, coord):\n",
    "        row, col = edge_index\n",
    "        coord_diff = coord[row] - coord[col]\n",
    "        radial = torch.sum(coord_diff**2, 1).unsqueeze(1)\n",
    "\n",
    "        if self.normalize:\n",
    "            norm = torch.sqrt(radial).detach() + self.epsilon\n",
    "            coord_diff = coord_diff / norm\n",
    "\n",
    "        return radial, coord_diff\n",
    "\n",
    "    def forward(self, h, edge_index, coord, edge_attr=None, node_attr=None):\n",
    "        row, col = edge_index\n",
    "        radial, coord_diff = self.coord2radial(edge_index, coord)\n",
    "\n",
    "        edge_feat = self.edge_model(h[row], h[col], radial, edge_attr)\n",
    "        coord = self.coord_model(coord, edge_index, coord_diff, edge_feat)\n",
    "        h, agg = self.node_model(h, edge_index, edge_feat, node_attr)\n",
    "\n",
    "        return h, coord\n",
    "\n",
    "    \n",
    "    \n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    result_shape = (num_segments, data.size(1))\n",
    "    segment_ids = segment_ids.unsqueeze(-1).expand(-1, data.size(1))\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    count = data.new_full(result_shape, 0)\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    count.scatter_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dace6b4-8c00-4810-821c-9fd4f564da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import voxel_grid, max_pool, max_pool_x, global_mean_pool\n",
    "\n",
    "class egnn1(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(egnn1, self).__init__()\n",
    "        \n",
    "        self.conv1 = E_GCL(2, 32, 64, 2, residual=False)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.conv2 = E_GCL(32, 64, 128, 2, residual=False)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.conv3 = E_GCL(64, 128, 150, 2, residual=False)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(128 + 2, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 128)\n",
    "        self.fc3 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        data.x, data.pos = self.conv1(data.x, data.edge_index, data.pos, data.edge_attr)\n",
    "        data.x, data.pos = F.elu(data.x), F.elu(data.pos)\n",
    "        data.x = self.bn1(data.x)\n",
    "        \n",
    "        data.x, data.pos = self.conv2(data.x, data.edge_index, data.pos, data.edge_attr)\n",
    "        data.x, data.pos = F.elu(data.x), F.elu(data.pos)\n",
    "        data.x = self.bn2(data.x)\n",
    "        \n",
    "        data.x, data.pos = self.conv3(data.x, data.edge_index, data.pos, data.edge_attr)\n",
    "        data.x, data.pos = F.elu(data.x), F.elu(data.pos)\n",
    "        data.x = self.bn3(data.x)\n",
    "\n",
    "        x = global_mean_pool(data.x, data.batch)\n",
    "        pos = global_mean_pool(data.pos, data.batch)\n",
    "        x = torch.hstack([pos, x])\n",
    "        \n",
    "        x = x.view(-1, self.fc1.weight.size(1))\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c797217f-066f-4732-952a-3c029fe6a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = egnn1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06cd28e2-18fe-44f9-b9fd-352578f79389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "egnn1(\n",
       "  (conv1): E_GCL(\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=7, out_features=64, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (3): SiLU()\n",
       "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    )\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=66, out_features=64, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (3): SiLU()\n",
       "      (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "    )\n",
       "    (coord_mlp): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=64, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): E_GCL(\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=67, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (3): SiLU()\n",
       "      (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    )\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=160, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (3): SiLU()\n",
       "      (4): Linear(in_features=256, out_features=64, bias=True)\n",
       "    )\n",
       "    (coord_mlp): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=128, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): E_GCL(\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=131, out_features=150, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=150, out_features=300, bias=True)\n",
       "      (3): SiLU()\n",
       "      (4): Linear(in_features=300, out_features=150, bias=True)\n",
       "    )\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=214, out_features=150, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=150, out_features=300, bias=True)\n",
       "      (3): SiLU()\n",
       "      (4): Linear(in_features=300, out_features=128, bias=True)\n",
       "    )\n",
       "    (coord_mlp): Sequential(\n",
       "      (0): Linear(in_features=150, out_features=150, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Linear(in_features=150, out_features=1, bias=False)\n",
       "    )\n",
       "  )\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=130, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab7a819-efa4-49ff-8832-2fd70b14de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.ExtractContactCases import ExtractContactCases\n",
    "\n",
    "ex=ExtractContactCases('/home/hussain/me/projects/tactile/data/contact_extraction1', bag_file_name='../data/dataset_ENVTACT_new2.bag', _keep_raw=True)\n",
    "#ex.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fd222eb-2d49-4aaf-be61-5c6cb5615a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports.TrainModel import TrainModel\n",
    "from torch_geometric.transforms import Distance, Cartesian, Center, Compose\n",
    "from torch_geometric import seed_everything\n",
    "seed_everything(0)\n",
    "#!rm ../data/contact_extraction1/{train,test,val}/processed/*\n",
    "\n",
    "tm = TrainModel(\n",
    "    '../data/contact_extraction1/', \n",
    "    model.to('cuda'), \n",
    "    n_epochs=150, \n",
    "    transform=Compose([Center(), Cartesian(cat=False)]), \n",
    "    features='pol_time', \n",
    "    lr = 0.001,\n",
    "    weight_decay=0.005,\n",
    "    augment=False, patience=10, batch=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d257e3-c916-4ba5-92f0-a46fd7d2f7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab2b004b9de40ab813f0cb4481b1be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training:   0%|          | 0/150 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3404ee64511d4866b6c5d8bd02641e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/97 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hussain/me/projects/tactile/src/imports/TrainModel.py:72: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
      "  with torch.autograd.detect_anomaly():\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 3.82 GiB total capacity; 2.38 GiB already allocated; 108.62 MiB free; 2.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/me/projects/tactile/src/imports/TrainModel.py:75\u001b[0m, in \u001b[0;36mTrainModel.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 75\u001b[0m end_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_func(end_point, data\u001b[38;5;241m.\u001b[39my)\n\u001b[1;32m     77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36megnn1.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     27\u001b[0m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(data\u001b[38;5;241m.\u001b[39mx), F\u001b[38;5;241m.\u001b[39melu(data\u001b[38;5;241m.\u001b[39mpos)\n\u001b[1;32m     28\u001b[0m data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(data\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m---> 30\u001b[0m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(data\u001b[38;5;241m.\u001b[39mx), F\u001b[38;5;241m.\u001b[39melu(data\u001b[38;5;241m.\u001b[39mpos)\n\u001b[1;32m     32\u001b[0m data\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(data\u001b[38;5;241m.\u001b[39mx)\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mE_GCL.forward\u001b[0;34m(self, h, edge_index, coord, edge_attr, node_attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m radial, coord_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord2radial(edge_index, coord)\n\u001b[1;32m    116\u001b[0m edge_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_model(h[row], h[col], radial, edge_attr)\n\u001b[0;32m--> 117\u001b[0m coord \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoord_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoord\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoord_diff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m h, agg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_model(h, edge_index, edge_feat, node_attr)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h, coord\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mE_GCL.coord_model\u001b[0;34m(self, coord, edge_index, coord_diff, edge_feat)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcoord_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, coord, edge_index, coord_diff, edge_feat):\n\u001b[1;32m     90\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m edge_index\n\u001b[0;32m---> 91\u001b[0m     trans \u001b[38;5;241m=\u001b[39m coord_diff \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoord_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_feat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoords_agg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     93\u001b[0m         agg \u001b[38;5;241m=\u001b[39m unsorted_segment_sum(trans, row, num_segments\u001b[38;5;241m=\u001b[39mcoord\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/modules/activation.py:391\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tactile/lib/python3.8/site-packages/torch/nn/functional.py:2048\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   2047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m-> 2048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 0; 3.82 GiB total capacity; 2.38 GiB already allocated; 108.62 MiB free; 2.96 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "tm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f72b0ba9-fe18-41a1-87f0-0af4efc0d62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fb06d69a820>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc71bf5-5f9f-4242-8eec-19198d56e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8519fddf-ed59-4163-b403-a8f52212c14d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
